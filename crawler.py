import requests
from bs4 import BeautifulSoup

def get_mju_notices(category_code: str = "255", limit: int = 8):
    """
    ëª…ì§€ëŒ€í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬ (í…ìŠ¤íŠ¸/ì½”ë“œ ìë™ ë³€í™˜ ê¸°ëŠ¥ íƒ‘ì¬)
    """
    
    # ---------------------------------------------------------
    # [í•µì‹¬ ìˆ˜ì •] Geminiê°€ í•œêµ­ì–´ë¡œ ë³´ë‚´ë„ ìˆ«ìë¡œ ë°”ê¿”ì£¼ëŠ” ë§¤í•‘ í…Œì´ë¸”
    # ---------------------------------------------------------
    mapping = {
        # ì¼ë°˜ ê³µì§€ ê´€ë ¨
        "ì¼ë°˜": "255", "ê³µì§€": "255", "255": "255",
        
        # í•™ì‚¬ ê³µì§€ ê´€ë ¨
        "í•™ì‚¬": "257", "ìˆ˜ê°•": "257", "ì¡¸ì—…": "257", "íœ´í•™": "257", "ë³µí•™": "257", "257": "257",
        
        # ì¥í•™ ê³µì§€ ê´€ë ¨
        "ì¥í•™": "259", "í•™ìê¸ˆ": "259", "ëŒ€ì¶œ": "259", "259": "259",
        
        # ì·¨ì—… ê³µì§€ ê´€ë ¨
        "ì·¨ì—…": "260", "ì§„ë¡œ": "260", "ì°½ì—…": "260", "ì¸í„´": "260", "í˜„ì¥": "260", "260": "260"
    }

    # 1. ì…ë ¥ê°’ ì •ë¦¬ (ê³µë°± ì œê±° ë“±)
    clean_input = str(category_code).strip()
    
    # 2. ë§¤í•‘ í…Œì´ë¸”ì—ì„œ ì ì ˆí•œ ì½”ë“œ ì°¾ê¸°
    # ê¸°ë³¸ê°’ì€ '255'(ì¼ë°˜ê³µì§€)ë¡œ ì„¤ì •
    target_code = "255" 
    
    for key, val in mapping.items():
        if key in clean_input:
            target_code = val
            break
            
    # ---------------------------------------------------------

    # ë””ë²„ê¹…ìš© ì¶œë ¥
    board_names = {"255": "ì¼ë°˜ê³µì§€", "257": "í•™ì‚¬ê³µì§€", "259": "ì¥í•™ê³µì§€", "260": "ì·¨ì°½ì—…ê³µì§€"}
    board_name = board_names.get(target_code, "ì¼ë°˜ê³µì§€")
    
    print(f"ğŸ•µï¸ [Crawler] '{clean_input}' -> '{target_code}({board_name})'ë¡œ ë³€í™˜í•˜ì—¬ ì ‘ì† ì¤‘...")
    
    url = f"https://www.mju.ac.kr/mjukr/{target_code}/subview.do"
    
    try:
        response = requests.get(url, timeout=10)
        soup = BeautifulSoup(response.text, 'html.parser')
        rows = soup.select("tbody tr")
        
        print(f"ğŸ“Š [Debug] ë°œê²¬ëœ ì „ì²´ í–‰(Row) ìˆ˜: {len(rows)}")

        notices = []
        for row in rows:
            columns = row.select("td")
            if len(columns) < 2: continue

            # ì œëª© ì°¾ê¸° (2ë²ˆì§¸ or 3ë²ˆì§¸ ì¹¸)
            title_col = columns[1]
            link_tag = title_col.select_one("a")
            if not link_tag and len(columns) > 2:
                title_col = columns[2]
                link_tag = title_col.select_one("a")

            if link_tag:
                # í…ìŠ¤íŠ¸ ë‹¤ë¦¼ì§ˆ (ê³µë°±/ì—”í„° ì œê±°)
                raw_text = link_tag.text
                title = " ".join(raw_text.split())
                
                link = link_tag['href']
                if link.startswith("/"):
                    link = "https://www.mju.ac.kr" + link
                
                date = "ë‚ ì§œì—†ìŒ"
                for col in columns[2:]:
                    text = col.text.strip()
                    if len(text) == 10 and "." in text:
                        date = text
                        break

                notices.append(f"- [{date}] {title}\n  (ë§í¬: {link})")
            
            if len(notices) >= limit:
                break
        
        if not notices:
            return f"'{board_name}'ì—ì„œ ê³µì§€ì‚¬í•­ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (URL: {url})"

        result_text = "\n".join(notices)
        return f"ëª…ì§€ëŒ€í•™êµ {board_name} ìµœì‹  ëª©ë¡ì…ë‹ˆë‹¤ ({len(notices)}ê°œ):\n\n{result_text}"

    except Exception as e:
        print(f"âŒ [Crawler] ì—ëŸ¬ ë°œìƒ: {e}")
        return f"ì—ëŸ¬ ë°œìƒ: {str(e)}"